{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4d9cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7284bf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language                                           Question  \\\n",
      "0  English      What does “þeod-cyninga, þrym gefrunon” mean？   \n",
      "1  English  What does “egsode eorl. Syððan ærest wearð ” m...   \n",
      "2  English                         What does “Sōþlīċe ” mean？   \n",
      "3  English  And ic cyðe eow, þæt ic wylle beon hold hlafor...   \n",
      "4  English            What does “ \\tTōbecume þīn rīċe,” mean？   \n",
      "\n",
      "                                     Standard Answer Period  \n",
      "0  [of] people-kings, trim (glory) afrained (have...    NaN  \n",
      "1  awed earls (leaders of men). Sith (since) erst...    NaN  \n",
      "2                                            Soothly    NaN  \n",
      "3  And I kithe(make known/couth to) you, that I w...    NaN  \n",
      "4                          Come thy riche (kingdom),    NaN  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"/Users/wenzhuochen/Desktop/diaxlms/term paper/dataset1/Dataset1.xlsx\")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73c5f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df[['Language', 'Question','Standard Answer']]\n",
    "\n",
    "# Specify the file path for the output JSON file\n",
    "json_file_path = \"/Users/wenzhuochen/Desktop/diaxlms/term paper/Dataset1.json\"\n",
    "\n",
    "# Convert the DataFrame to a JSON file\n",
    "df_selected.to_json(json_file_path, orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e04d6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('dataset1.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON from line: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a87dd5",
   "metadata": {},
   "source": [
    "Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f2c6e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = []\n",
    "openai.api_key = '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'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ed7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    # Check if 'Question' exists and is not None\n",
    "    if 'Question' in item and item['Question'] is not None:\n",
    "        response = openai.ChatCompletion.create(\n",
    "          model=\"gpt-3.5-turbo\",\n",
    "          messages=[\n",
    "                {\"role\": \"user\", \"content\": item[\"Question\"]}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Assuming response structure is correct and response is successful\n",
    "        item[\"Generated Answer\"] = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        # Handle the case where 'Question' is missing or null\n",
    "        item[\"Generated Answer\"] = \"Question not provided or is null\"\n",
    "\n",
    "    generated_data.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b44fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(generated_data)\n",
    "\n",
    "csv_file_path = 'generated_data.csv'  \n",
    "df.to_csv('/Users/wenzhuochen/Desktop/diaxlms/term paper/generated_data.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd62a2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabbf8c6ca85485aa189be68927400c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba68dbfd06924213a18556981ba53b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.68 seconds, 11.91 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af56afe1b5d4c589d9dfbd81bb1e062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637c0e6e4f344615a8284d110ee3d465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.32 seconds, 8.63 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e372902bf5f04c4ea31582fc19e6ab7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bcf154d4004528a6cb0bb19f0818c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.50 seconds, 7.99 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8a951fe94f41eb811be38e016f5af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc54e259c6464a83a41f9f9000225965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.01 seconds, 9.93 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3aa1a8dba294d0f9a2e2843ed4f0ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042d657724f74973ab86cd73d28ae560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.80 seconds, 11.10 sentences/sec\n",
      "English - Precision: 0.6237559914588928, Recall: 0.7235369682312012, F1: 0.6679812669754028\n",
      "Chinese - Precision: 0.6738039255142212, Recall: 0.7312484979629517, F1: 0.700900673866272\n",
      "Spain - Precision: 0.6311453580856323, Recall: 0.7251454591751099, F1: 0.6737183332443237\n",
      "German - Precision: 0.6044057011604309, Recall: 0.742156445980072, F1: 0.6652911305427551\n",
      "French - Precision: 0.6584790945053101, Recall: 0.744273841381073, F1: 0.6974011063575745\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "\n",
    "# 结果字典\n",
    "results = {}\n",
    "\n",
    "# 对每种语言单独处理\n",
    "for language in df['Language'].unique():\n",
    "    language_data = df[df['Language'] == language]\n",
    "    cands = language_data['Generated Answer'].tolist()\n",
    "    refs = language_data['Standard Answer'].tolist()\n",
    "    \n",
    "    # 计算BERTScore\n",
    "    P, R, F1 = score(cands, refs, lang=language, verbose=True)\n",
    "    \n",
    "    # 存储平均分数\n",
    "    results[language] = {\n",
    "        'Precision': P.mean().item(),\n",
    "        'Recall': R.mean().item(),\n",
    "        'F1': F1.mean().item()\n",
    "    }\n",
    "\n",
    "# 打印结果\n",
    "for language, scores in results.items():\n",
    "    print(f\"{language} - Precision: {scores['Precision']}, Recall: {scores['Recall']}, F1: {scores['F1']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f50f9",
   "metadata": {},
   "source": [
    "Phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2996c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ifcorrect_data = []  # Corrected variable name\n",
    "\n",
    "for item in data:\n",
    "    if 'Question' in item and item['Question'] is not None:\n",
    "        # Constructing a prompt to ask the model to evaluate an answer\n",
    "        prompt = f\"Given the question '{item['Question']}', is the following answer correct? '{item['Standard Answer']}' Reply with 0 (wrong) or 1 (right) without a period.\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        # Assuming response structure is correct and response is successful\n",
    "        item[\"Ifcorrect\"] = response[\"choices\"][0][\"message\"][\"content\"].strip()  # Stripping to clean the response\n",
    "    else:\n",
    "        # Case where 'Question' is missing or null\n",
    "        item[\"Ifcorrect\"] = \"Question not provided or is null\"\n",
    "\n",
    "    Ifcorrect_data.append(item)  # Corrected list name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16304752",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Ifcorrect_data)\n",
    "\n",
    "# Define the path where you want to save the CSV file\n",
    "csv_file_path = '/Users/wenzhuochen/Desktop/diaxlms/term paper/ifcorrect_data.csv'\n",
    "# Save the DataFrame to CSV\n",
    "df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2cca048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Language:\n",
      "Language\n",
      "Chinese    1.00\n",
      "English    0.95\n",
      "French     1.00\n",
      "German     1.00\n",
      "Spain      0.90\n",
      "Name: Ifcorrect, dtype: float64\n",
      "\n",
      "Overall Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/wenzhuochen/Desktop/diaxlms/term paper/dataset1/ifcorrect_data.csv')\n",
    "\n",
    "accuracy_by_language = df.groupby('Language')['Ifcorrect'].mean()\n",
    "overall_accuracy = df['Ifcorrect'].mean()\n",
    "\n",
    "print(\"Accuracy by Language:\")\n",
    "print(accuracy_by_language)\n",
    "\n",
    "print(\"\\nOverall Accuracy:\", overall_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec3341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
